{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Target Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>role cate</th>\n",
       "      <th>occupation cate</th>\n",
       "      <th>merged_cate</th>\n",
       "      <th>N-gram Frequency (2018-2019)</th>\n",
       "      <th>mmlu</th>\n",
       "      <th>interpersonal</th>\n",
       "      <th>gender</th>\n",
       "      <th>align_words</th>\n",
       "      <th>gender_role_cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psychologist</td>\n",
       "      <td>work</td>\n",
       "      <td>psychology</td>\n",
       "      <td>psychology</td>\n",
       "      <td>4.646573e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politician</td>\n",
       "      <td>work</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>5.218259e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sheriff</td>\n",
       "      <td>work</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>7.653504e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>governer</td>\n",
       "      <td>work</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>3.078719e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>geneticist</td>\n",
       "      <td>work</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>2.580220e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>biologist</td>\n",
       "      <td>work</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>1.110472e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>physicist</td>\n",
       "      <td>work</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>2.030176e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>teacher</td>\n",
       "      <td>school</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>5.522994e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>chemist</td>\n",
       "      <td>work</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>1.986987e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ecologist</td>\n",
       "      <td>work</td>\n",
       "      <td>natural science</td>\n",
       "      <td>natural science</td>\n",
       "      <td>2.505230e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nurse</td>\n",
       "      <td>work</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "      <td>2.494348e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>doctor</td>\n",
       "      <td>work</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "      <td>5.117561e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>physician</td>\n",
       "      <td>work</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "      <td>1.669908e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>dentist</td>\n",
       "      <td>work</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "      <td>2.256031e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>surgeon</td>\n",
       "      <td>work</td>\n",
       "      <td>medicine</td>\n",
       "      <td>medicine</td>\n",
       "      <td>9.010343e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>data analyst</td>\n",
       "      <td>work</td>\n",
       "      <td>math</td>\n",
       "      <td>math</td>\n",
       "      <td>6.469216e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mathematician</td>\n",
       "      <td>work</td>\n",
       "      <td>math</td>\n",
       "      <td>math</td>\n",
       "      <td>1.991564e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>statistician</td>\n",
       "      <td>work</td>\n",
       "      <td>math</td>\n",
       "      <td>math</td>\n",
       "      <td>3.983013e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>bailiff</td>\n",
       "      <td>work</td>\n",
       "      <td>law</td>\n",
       "      <td>law</td>\n",
       "      <td>9.807435e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>work</td>\n",
       "      <td>law</td>\n",
       "      <td>law</td>\n",
       "      <td>2.045239e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>historian</td>\n",
       "      <td>work</td>\n",
       "      <td>history</td>\n",
       "      <td>history</td>\n",
       "      <td>1.062402e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>archivist</td>\n",
       "      <td>work</td>\n",
       "      <td>history</td>\n",
       "      <td>history</td>\n",
       "      <td>3.925437e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>historical researcher</td>\n",
       "      <td>work</td>\n",
       "      <td>history</td>\n",
       "      <td>history</td>\n",
       "      <td>5.191555e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>archaeologist</td>\n",
       "      <td>work</td>\n",
       "      <td>history</td>\n",
       "      <td>history</td>\n",
       "      <td>8.446297e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>electronics technician</td>\n",
       "      <td>work</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>7.011236e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>work</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>1.431244e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>electrical engineer</td>\n",
       "      <td>work</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>1.304718e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>software engineer</td>\n",
       "      <td>work</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>1.363839e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>web developer</td>\n",
       "      <td>work</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>3.648814e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>economic researcher</td>\n",
       "      <td>work</td>\n",
       "      <td>econ</td>\n",
       "      <td>econ</td>\n",
       "      <td>1.304612e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>economist</td>\n",
       "      <td>work</td>\n",
       "      <td>econ</td>\n",
       "      <td>econ</td>\n",
       "      <td>2.738544e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>financial analyst</td>\n",
       "      <td>work</td>\n",
       "      <td>econ</td>\n",
       "      <td>econ</td>\n",
       "      <td>6.151316e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>enthusiast</td>\n",
       "      <td>work</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>9.079468e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>partisan</td>\n",
       "      <td>work</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>4.140323e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Embedded Systems AI Engineer</td>\n",
       "      <td>AI</td>\n",
       "      <td>eecs</td>\n",
       "      <td>eecs</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             role role cate  occupation cate      merged_cate  \\\n",
       "2                    psychologist      work       psychology       psychology   \n",
       "6                      politician      work         politics         politics   \n",
       "7                         sheriff      work         politics         politics   \n",
       "9                        governer      work         politics         politics   \n",
       "10                     geneticist      work  natural science  natural science   \n",
       "19                      biologist      work  natural science  natural science   \n",
       "20                      physicist      work  natural science  natural science   \n",
       "21                        teacher    school  natural science  natural science   \n",
       "22                        chemist      work  natural science  natural science   \n",
       "26                      ecologist      work  natural science  natural science   \n",
       "28                          nurse      work         medicine         medicine   \n",
       "29                         doctor      work         medicine         medicine   \n",
       "30                      physician      work         medicine         medicine   \n",
       "33                        dentist      work         medicine         medicine   \n",
       "34                        surgeon      work         medicine         medicine   \n",
       "35                   data analyst      work             math             math   \n",
       "37                  mathematician      work             math             math   \n",
       "41                   statistician      work             math             math   \n",
       "47                        bailiff      work              law              law   \n",
       "48                         lawyer      work              law              law   \n",
       "49                      historian      work          history          history   \n",
       "50                      archivist      work          history          history   \n",
       "51          historical researcher      work          history          history   \n",
       "57                  archaeologist      work          history          history   \n",
       "58         electronics technician      work             eecs             eecs   \n",
       "60                 data scientist      work             eecs             eecs   \n",
       "62            electrical engineer      work             eecs             eecs   \n",
       "68              software engineer      work             eecs             eecs   \n",
       "80                  web developer      work             eecs             eecs   \n",
       "81            economic researcher      work             econ             econ   \n",
       "82                      economist      work             econ             econ   \n",
       "84              financial analyst      work             econ             econ   \n",
       "102                    enthusiast      work         politics         politics   \n",
       "104                      partisan      work         politics         politics   \n",
       "152  Embedded Systems AI Engineer        AI             eecs             eecs   \n",
       "\n",
       "     N-gram Frequency (2018-2019)  mmlu  interpersonal   gender align_words  \\\n",
       "2                    4.646573e-06     1              0  unknown         NaN   \n",
       "6                    5.218259e-06     1              0  unknown         NaN   \n",
       "7                    7.653504e-06     1              0  unknown         NaN   \n",
       "9                    3.078719e-09     1              0  unknown         NaN   \n",
       "10                   2.580220e-07     1              0  unknown         NaN   \n",
       "19                   1.110472e-06     1              0  unknown         NaN   \n",
       "20                   2.030176e-06     1              0  unknown         NaN   \n",
       "21                   5.522994e-05     1              0  unknown         NaN   \n",
       "22                   1.986987e-06     1              0  unknown         NaN   \n",
       "26                   2.505230e-07     1              0  unknown         NaN   \n",
       "28                   2.494348e-05     1              0  unknown         NaN   \n",
       "29                   5.117561e-05     1              0  unknown         NaN   \n",
       "30                   1.669908e-05     1              0  unknown         NaN   \n",
       "33                   2.256031e-06     1              0  unknown         NaN   \n",
       "34                   9.010343e-06     1              0  unknown         NaN   \n",
       "35                   6.469216e-08     1              0  unknown         NaN   \n",
       "37                   1.991564e-06     1              0  unknown         NaN   \n",
       "41                   3.983013e-07     1              0  unknown         NaN   \n",
       "47                   9.807435e-07     1              0  unknown         NaN   \n",
       "48                   2.045239e-05     1              0  unknown         NaN   \n",
       "49                   1.062402e-05     1              0  unknown         NaN   \n",
       "50                   3.925437e-07     1              0  unknown         NaN   \n",
       "51                   5.191555e-09     1              0  unknown         NaN   \n",
       "57                   8.446297e-07     1              0  unknown         NaN   \n",
       "58                   7.011236e-09     1              0  unknown         NaN   \n",
       "60                   1.431244e-07     1              0  unknown         NaN   \n",
       "62                   1.304718e-07     1              0  unknown         NaN   \n",
       "68                   1.363839e-07     1              0  unknown         NaN   \n",
       "80                   3.648814e-08     1              0  unknown         NaN   \n",
       "81                   1.304612e-09     1              0  unknown         NaN   \n",
       "82                   2.738544e-06     1              0  unknown         NaN   \n",
       "84                   6.151316e-08     1              0  unknown         NaN   \n",
       "102                  9.079468e-07     1              0  unknown         NaN   \n",
       "104                  4.140323e-06     1              0  unknown         NaN   \n",
       "152                  0.000000e+00     1              0  unknown         NaN   \n",
       "\n",
       "    gender_role_cate  \n",
       "2                NaN  \n",
       "6                NaN  \n",
       "7                NaN  \n",
       "9                NaN  \n",
       "10               NaN  \n",
       "19               NaN  \n",
       "20               NaN  \n",
       "21               NaN  \n",
       "22               NaN  \n",
       "26               NaN  \n",
       "28               NaN  \n",
       "29               NaN  \n",
       "30               NaN  \n",
       "33               NaN  \n",
       "34               NaN  \n",
       "35               NaN  \n",
       "37               NaN  \n",
       "41               NaN  \n",
       "47               NaN  \n",
       "48               NaN  \n",
       "49               NaN  \n",
       "50               NaN  \n",
       "51               NaN  \n",
       "57               NaN  \n",
       "58               NaN  \n",
       "60               NaN  \n",
       "62               NaN  \n",
       "68               NaN  \n",
       "80               NaN  \n",
       "81               NaN  \n",
       "82               NaN  \n",
       "84               NaN  \n",
       "102              NaN  \n",
       "104              NaN  \n",
       "152              NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from generator import Generator\n",
    "current_dir = os.path.abspath(\"\")\n",
    "processed_data_dir = os.path.join(current_dir, 'processed')\n",
    "splits_data_dir = os.path.join(current_dir, 'splits')\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# # Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.CRITICAL,  # Set the default logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('personas.log'),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the API key and the model repository\n",
    "OPENROUTER_KEY = os.environ.get('OPENROUTER_KEY')\n",
    "repository = 'anthropic/claude-3.5-sonnet'\n",
    "model = repository.split('/')[1]\n",
    "providers = ['Anthropic']\n",
    "generator = Generator(repository, OPENROUTER_KEY, providers=providers,)\n",
    "\n",
    "# Download role info data from GitHub\n",
    "url = 'https://raw.githubusercontent.com/Jiaxin-Pei/Prompting-with-Social-Roles/refs/heads/main/data/role_info.csv'\n",
    "roles_info = pd.read_csv(url)\n",
    "\n",
    "# We are interested only in the roles that are present in mmlu\n",
    "roles_info = roles_info[roles_info['mmlu'] == 1]\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/tencent-ailab/persona-hub/refs/heads/main/data/persona.jsonl'\n",
    "personas = pd.read_json(url, lines=True)\n",
    "\n",
    "roles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per individual role:\n",
      "      Split       Role  Count\n",
      "0  medicine      nurse   1206\n",
      "1  medicine     doctor   1094\n",
      "2  medicine  physician    279\n",
      "3  medicine    dentist    101\n",
      "4  medicine    surgeon    306\n",
      "\n",
      "Overall (complessivo) statistics across all roles:\n",
      "       min_count  max_count  mean_count  total_count\n",
      "Count      101.0     1206.0       597.2       2986.0\n"
     ]
    }
   ],
   "source": [
    "# Define the roles dictionary mapping split to its associated role strings\n",
    "roles_dict = {\n",
    "    # \"econ\": [\"economic researcher\", \"economist\", \"financial analyst\"],\n",
    "    # \"eecs\": [\"electronics technician\", \"data scientist\", \"electrical engineer\", \"software engineer\", \"web developer\"],\n",
    "    # \"history\": [\"historian\", \"archivist\", \"historical researcher\", \"archaeologist\"],\n",
    "    # \"law\": [\"bailiff\", \"lawyer\"],\n",
    "    # \"math\": [\"data analyst\", \"mathematician\", \"statistician\"],\n",
    "    \"medicine\": [\"nurse\", \"doctor\", \"physician\", \"dentist\", \"surgeon\"],\n",
    "    # \"natural science\": [\"geneticist\", \"biologist\", \"physicist\", \"teacher\", \"chemist\", \"ecologist\"],\n",
    "    # \"politics\": [\"politician\", \"sheriff\", \"enthusiast\", \"partisan\"],\n",
    "    # \"psychology\": [\"psychologist\"],\n",
    "}\n",
    "\n",
    "# Prepare a list to store counts for each individual role.\n",
    "# (We assume that the persona hub data has a column 'persona' containing text.)\n",
    "results = []\n",
    "for split, roles in roles_dict.items():\n",
    "    for role in roles:\n",
    "        # Use case-insensitive matching (skip NaNs with na=False)\n",
    "        mask = personas['persona'].str.lower().str.contains(role.lower(), na=False)\n",
    "        count = mask.sum()\n",
    "        results.append({\n",
    "            \"Split\": split,\n",
    "            \"Role\": role,\n",
    "            \"Count\": count\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Counts per individual role:\")\n",
    "print(results_df)\n",
    "\n",
    "# --- Overall (complessivo) statistics across all roles ---\n",
    "overall_stats = results_df[\"Count\"].agg(\n",
    "    min_count=\"min\",\n",
    "    max_count=\"max\",\n",
    "    mean_count=\"mean\",\n",
    "    total_count=\"sum\"\n",
    ")\n",
    "overall_stats_df = pd.DataFrame([overall_stats])\n",
    "print(\"\\nOverall (complessivo) statistics across all roles:\")\n",
    "print(overall_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: psychologist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:08<00:00, 14.34it/s]\n",
      "  3%|▎         | 1/35 [00:08<05:04,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: politician\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 28.78it/s]\n",
      "  6%|▌         | 2/35 [00:13<03:28,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: sheriff\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 26.24it/s]\n",
      "  9%|▊         | 3/35 [00:18<03:02,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: governer\n",
      "No personas found for role: governer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:03<00:00, 33.73it/s]\n",
      " 11%|█▏        | 4/35 [00:22<02:33,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: geneticist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:03<00:00, 32.15it/s]\n",
      " 14%|█▍        | 5/35 [00:26<02:18,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: biologist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:05<00:00, 24.83it/s]\n",
      " 17%|█▋        | 6/35 [00:31<02:19,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: physicist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:05<00:00, 24.13it/s]\n",
      " 20%|██        | 7/35 [00:36<02:19,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: teacher\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 26.89it/s]\n",
      " 23%|██▎       | 8/35 [00:41<02:13,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: chemist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:09<00:00, 13.58it/s]\n",
      " 26%|██▌       | 9/35 [00:51<02:45,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: ecologist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:09<00:00, 13.25it/s]\n",
      " 29%|██▊       | 10/35 [01:00<03:04,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: nurse\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:06<00:00, 21.06it/s]\n",
      " 31%|███▏      | 11/35 [01:06<02:47,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: doctor\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:09<00:00, 13.52it/s]\n",
      " 34%|███▍      | 12/35 [01:16<02:58,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: physician\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:05<00:00, 23.19it/s]\n",
      " 37%|███▋      | 13/35 [01:21<02:36,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: dentist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 30.00it/s]\n",
      " 40%|████      | 14/35 [01:26<02:11,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: surgeon\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 31.31it/s]\n",
      " 43%|████▎     | 15/35 [01:30<01:52,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: data analyst\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:05<00:00, 23.64it/s]\n",
      " 46%|████▌     | 16/35 [01:35<01:45,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: mathematician\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 26.82it/s]\n",
      " 49%|████▊     | 17/35 [01:40<01:36,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: statistician\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 29.23it/s]\n",
      " 51%|█████▏    | 18/35 [01:45<01:26,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: bailiff\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 27.04it/s]\n",
      " 54%|█████▍    | 19/35 [01:49<01:19,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: lawyer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 26.87it/s]\n",
      " 57%|█████▋    | 20/35 [01:54<01:13,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: historian\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 31.87it/s]\n",
      " 60%|██████    | 21/35 [01:58<01:05,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: archivist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:09<00:00, 14.20it/s]\n",
      " 63%|██████▎   | 22/35 [02:07<01:17,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: historical researcher\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:08<00:00, 15.37it/s]\n",
      " 66%|██████▌   | 23/35 [02:16<01:20,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: archaeologist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:08<00:00, 14.76it/s]\n",
      " 69%|██████▊   | 24/35 [02:24<01:20,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: electronics technician\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 31.28it/s]\n",
      " 71%|███████▏  | 25/35 [02:29<01:03,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: data scientist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 31.91it/s]\n",
      " 74%|███████▍  | 26/35 [02:33<00:51,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: electrical engineer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:03<00:00, 34.91it/s]\n",
      " 77%|███████▋  | 27/35 [02:36<00:40,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: software engineer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 28.70it/s]\n",
      " 80%|████████  | 28/35 [02:41<00:34,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: web developer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:08<00:00, 14.27it/s]\n",
      " 83%|████████▎ | 29/35 [02:50<00:36,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: economic researcher\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:09<00:00, 13.58it/s]\n",
      " 86%|████████▌ | 30/35 [02:59<00:35,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: economist\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:19<00:00,  6.58it/s]\n",
      " 89%|████████▊ | 31/35 [03:19<00:43, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: financial analyst\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:05<00:00, 24.86it/s]\n",
      " 91%|█████████▏| 32/35 [03:24<00:27,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: enthusiast\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:08<00:00, 15.66it/s]\n",
      " 94%|█████████▍| 33/35 [03:32<00:17,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: partisan\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:03<00:00, 34.10it/s]\n",
      " 97%|█████████▋| 34/35 [03:36<00:07,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts for role: Embedded Systems AI Engineer\n",
      "No personas found for role: Embedded Systems AI Engineer\n",
      "Generating 128 prompts with random personas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prompts: 100%|██████████| 128/128 [00:04<00:00, 31.14it/s]\n",
      "100%|██████████| 35/35 [03:40<00:00,  6.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import random\n",
    "\n",
    "task_types = [\n",
    "    \"describe\", \n",
    "    \"explain\",\n",
    "    \"design\",\n",
    "    \"what is\",\n",
    "    \"how to\",\n",
    "    \"analyze\",\n",
    "    \"compare\",\n",
    "    \"create\",\n",
    "    \"solve\",\n",
    "    \"recommend\"\n",
    "]\n",
    "\n",
    "async def generate_prompt_async(persona, i):\n",
    "    \"\"\"\n",
    "    Asynchronously generate a single prompt for a given persona.\n",
    "    \"\"\"\n",
    "    task_type = random.choice(task_types)\n",
    "    \n",
    "    instruction = f'''Generate a {task_type} prompt that this persona would likely ask:\n",
    "\n",
    "    Persona: {persona}\n",
    "\n",
    "    Rules:\n",
    "    1. The prompt should start with \"{task_type}\"\n",
    "    2. Keep it specific and under 15 words\n",
    "    3. Make it relevant to the persona's background/interests\n",
    "    4. Your output must start with \"User prompt:\"\n",
    "\n",
    "    Examples based on task types:\n",
    "    - describe: \"Describe the key features of a successful marketing campaign\"\n",
    "    - explain: \"Explain the process of setting up a home network\"\n",
    "    - design: \"Design a logo for a sustainable fashion brand\"\n",
    "    - what is: \"What is the difference between UI and UX design?\"\n",
    "    - how to: \"How to optimize a website for mobile devices?\"\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        raw_prompt = await generator.generate(instruction)\n",
    "        if 'User prompt:' in raw_prompt:\n",
    "            prompt = raw_prompt.split('User prompt:')[1].strip()\n",
    "        else:\n",
    "            prompt = raw_prompt.strip()\n",
    "        return (persona, prompt, task_type)  # Now also returning task_type\n",
    "    except Exception as e:\n",
    "        print(f'Error processing prompt {i}: {str(e)}')\n",
    "        return (persona, '', task_type)\n",
    "\n",
    "\n",
    "\n",
    "async def generate_prompts_async(personas_list, n=228):\n",
    "    \"\"\"\n",
    "    Asynchronously generate n prompts, each with a random persona.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for i in range(n):\n",
    "        # Randomly select a persona for each prompt\n",
    "        random_persona = random.choice(personas_list)\n",
    "        tasks.append(generate_prompt_async(random_persona, i))\n",
    "    \n",
    "    results = []\n",
    "    pbar = tqdm(total=n, desc=f'Generating prompts', leave=True)\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        try:\n",
    "            result = await task\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "        except Exception as e:\n",
    "            print(f'Error in task: {str(e)}')\n",
    "            results.append(('ERROR', ''))  # Append empty result in case of error\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "async def main():\n",
    "\n",
    "    # Loop through the roles and generate prompts for each role\n",
    "    # implement a loading to show the progress on roles\n",
    "    for role in tqdm(roles_info['role']):\n",
    "        print(f'Generating prompts for role: {role}')\n",
    "        # Setting the paths\n",
    "        prompts_file = f'raw/prompts_target_{model}_{role}.csv'\n",
    "\n",
    "        # Get the personas for the current role by filtering the personas DataFrame\n",
    "        # if a persona['persona'] string contains the role string\n",
    "\n",
    "        personas_list = personas[personas['persona'].str.contains(role)]['persona'].tolist()\n",
    "\n",
    "        if len(personas_list) == 0:\n",
    "            print(f'No personas found for role: {role}')\n",
    "            personas_list = [role]\n",
    "            prompts_file = f'raw/prompts_target_{model}_{role}_no_persona.csv'\n",
    "\n",
    "        # If file exists, skip generating prompts\n",
    "        if os.path.exists(prompts_file):\n",
    "            print(f'Prompts file already exists: {prompts_file}')\n",
    "            continue\n",
    "    \n",
    "        print(f'Generating {128} prompts with random personas')\n",
    "        results = await generate_prompts_async(personas_list, 128)\n",
    "        \n",
    "        # Update the DataFrame creation in main():\n",
    "        df = pd.DataFrame(results, columns=['persona', 'prompt', 'task_type'])\n",
    "        \n",
    "        # Save the results to a CSV file\n",
    "        # Ensure the directory exists if not create it\n",
    "        if not os.path.exists(os.path.dirname(prompts_file)):\n",
    "            os.makedirs(os.path.dirname(prompts_file))\n",
    "        df.to_csv(prompts_file, index=False)\n",
    "\n",
    "# Get the current event loop and run the async code\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(data, file_path):\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def download_generated_prompts(prompts_file, role):\n",
    "    # Load the prompts\n",
    "    prompts = pd.read_csv(prompts_file)\n",
    "    # convert filename into a json name\n",
    "    filename = os.path.basename(prompts_file)\n",
    "    filename = filename.replace('.csv', '.json')\n",
    "\n",
    "    processed_file_path = os.path.join(processed_data_dir, filename)\n",
    "    \n",
    "    instructions = prompts['prompt'].tolist()\n",
    "    # strip and remove \" from the instructions\n",
    "    instructions = [instruction.replace('\"', '').replace('*', '') for instruction in instructions]\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': role} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 630.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing role governer: [Errno 2] No such file or directory: 'raw/prompts_target_claude-3.5-sonnet_governer.csv'\n",
      "Error processing role Embedded Systems AI Engineer: [Errno 2] No such file or directory: 'raw/prompts_target_claude-3.5-sonnet_Embedded Systems AI Engineer.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for role in tqdm(roles_info['role']):\n",
    "    try:\n",
    "        prompts_file = f'raw/prompts_target_{model}_{role}.csv'\n",
    "        download_generated_prompts(prompts_file, role)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing role {role}: {str(e)}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Standard Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_file(url, file_path):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_alpaca():\n",
    "    processed_file_path = os.path.join(processed_data_dir, 'alpaca.json')\n",
    "\n",
    "    dataset = pd.read_csv('raw/alpaca.csv')\n",
    "\n",
    "    # filter for instructions that have empty inputs\n",
    "    mask = dataset['input'].isna() | (dataset['input'].str.strip() == '')\n",
    "    instructions = dataset.loc[mask, 'instruction'].tolist()\n",
    "\n",
    "    dataset_json = [{'instruction': instruction.strip(), 'category': None} for instruction in instructions]\n",
    "    dump_json(dataset_json, processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw/alpaca.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdownload_alpaca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mdownload_alpaca\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_alpaca\u001b[39m():\n\u001b[32m      2\u001b[39m     processed_file_path = os.path.join(processed_data_dir, \u001b[33m'\u001b[39m\u001b[33malpaca.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     dataset = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw/alpaca.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# filter for instructions that have empty inputs\u001b[39;00m\n\u001b[32m      7\u001b[39m     mask = dataset[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m].isna() | (dataset[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m].str.strip() == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\GitHub\\rolevectors\\rolevectors\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\GitHub\\rolevectors\\rolevectors\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\GitHub\\rolevectors\\rolevectors\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\GitHub\\rolevectors\\rolevectors\\env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Documents\\GitHub\\rolevectors\\rolevectors\\env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'raw/alpaca.csv'"
     ]
    }
   ],
   "source": [
    "download_alpaca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_base_dataset_splits():\n",
    "    base_train_path = os.path.join(splits_data_dir, 'base_train.json')\n",
    "    base_val_path = os.path.join(splits_data_dir, 'base_val.json')\n",
    "    base_test_path = os.path.join(splits_data_dir, 'base_test.json')\n",
    "\n",
    "    train_p, val_p, test_p = 0.6, 0.20, 0.20\n",
    "\n",
    "    base_instructions = []\n",
    "    for file in ['alpaca.json']:\n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            base_instructions.extend(json.load(f))\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(base_instructions)\n",
    "\n",
    "    total_size = len(base_instructions)\n",
    "    train_size = int(train_p * total_size)\n",
    "    val_size = int(val_p * total_size)\n",
    "\n",
    "    base_train_instructions = base_instructions[:train_size]\n",
    "    base_val_instructions = base_instructions[train_size:train_size+val_size]\n",
    "    base_test_instructions = base_instructions[train_size+val_size:]\n",
    "\n",
    "    dump_json(base_train_instructions, base_train_path)\n",
    "    dump_json(base_val_instructions, base_val_path)\n",
    "    dump_json(base_test_instructions, base_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_target_dataset_splits(model):\n",
    "\n",
    "    # Setting the paths\n",
    "    \n",
    "\n",
    "    #getting all the files that contain model string\n",
    "    files = [f for f in os.listdir(processed_data_dir) if model in f]\n",
    "\n",
    "    for file in files:\n",
    "        print(f'Generating splits for {file}')\n",
    "        # select target role from file name\n",
    "        #prompts_target_claude-3.5-sonnet_statistician.csv\n",
    "        target_role = file.split('_')[-1].replace('.csv', '')\n",
    "        target_train_path = os.path.join(splits_data_dir, f'target_train_{target_role}')\n",
    "\n",
    "        train_p  = 1\n",
    "\n",
    "        target_instructions = []\n",
    "        \n",
    "        with open(os.path.join(processed_data_dir, file), 'r') as f:\n",
    "            target_instructions.extend(json.load(f))\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(target_instructions)\n",
    "\n",
    "        total_size = len(target_instructions)\n",
    "        train_size = int(train_p * total_size)\n",
    "\n",
    "        target_train_instructions = target_instructions[:train_size]\n",
    "\n",
    "        dump_json(target_train_instructions, target_train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating splits for prompts_target_claude-3.5-sonnet_archaeologist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_archivist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_bailiff.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_biologist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_chemist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_data analyst.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_data scientist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_dentist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_doctor.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_ecologist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_economic researcher.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_economist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_electrical engineer.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_electronics technician.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_enthusiast.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_financial analyst.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_geneticist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_historian.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_historical researcher.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_lawyer.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_mathematician.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_nurse.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_partisan.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_physician.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_physicist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_politician.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_psychologist.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_sheriff.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_software engineer.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_statistician.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_surgeon.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_teacher.json\n",
      "Generating splits for prompts_target_claude-3.5-sonnet_web developer.json\n"
     ]
    }
   ],
   "source": [
    "# construct_base_dataset_splits()\n",
    "construct_target_dataset_splits(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(mmlu_data):\n",
    "    processed_data = []\n",
    "    \n",
    "    # Letter mapping for answers (0->A, 1->B, 2->C, 3->D)\n",
    "    letter_mapping = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
    "    letter_mapping_answer = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n",
    "    \n",
    "    for item in mmlu_data:\n",
    "        question = item['question']\n",
    "        answer = item['true_option']\n",
    "        choices = [item['option1'], item['option2'], item['option3'], item['option4']]\n",
    "        subject = item['subject']\n",
    "        \n",
    "        # Create the formatted choices string\n",
    "        formatted_choices = ''\n",
    "        for i, choice in enumerate(choices):\n",
    "            formatted_choices += f\"\\n\\t\\t\\t{letter_mapping[i]}. {choice}\"\n",
    "        \n",
    "        # Create the instruction string\n",
    "        instruction = (f\"{question}{formatted_choices}\\n\\t\\t\\t\"\n",
    "                      f\"Answer with the letter of the correct answer.\\n\\t\\t\\t\"\n",
    "                      f\"Answer:\")\n",
    "        \n",
    "        # Convert numeric answer to letter\n",
    "        target_score = letter_mapping_answer[answer]\n",
    "        \n",
    "        new_item = {\n",
    "            \"instruction\": instruction,\n",
    "            \"target_score\": target_score,\n",
    "            \"dataset\": subject,\n",
    "        }\n",
    "        processed_data.append(new_item)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def processing_mmlu_data(file):\n",
    "    mmlu_path = os.path.join(processed_data_dir, file)\n",
    "    mmlu_processed_path = os.path.join(splits_data_dir, file)\n",
    "\n",
    "    try:\n",
    "        with open(mmlu_path, 'r') as f:\n",
    "            mmlu = json.load(f)\n",
    "            \n",
    "        mmlu_processed = convert_format(mmlu)\n",
    "        print(f\"Processed {len(mmlu_processed)} MMLU examples\")\n",
    "        dump_json(mmlu_processed, mmlu_processed_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 590 MMLU examples\n",
      "Processed 492 MMLU examples\n",
      "Processed 247 MMLU examples\n",
      "Processed 200 MMLU examples\n",
      "Processed 287 MMLU examples\n",
      "Processed 241 MMLU examples\n",
      "Processed 200 MMLU examples\n",
      "Processed 200 MMLU examples\n"
     ]
    }
   ],
   "source": [
    "processing_mmlu_data(\"target_test_natural_science.json\")\n",
    "processing_mmlu_data(\"target_test_econ.json\")\n",
    "processing_mmlu_data(\"target_test_eecs.json\")\n",
    "processing_mmlu_data(\"target_test_law.json\")\n",
    "processing_mmlu_data(\"target_test_math.json\")\n",
    "processing_mmlu_data(\"target_test_medicine.json\")\n",
    "processing_mmlu_data(\"target_test_politics.json\")\n",
    "processing_mmlu_data(\"target_test_psychology.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
